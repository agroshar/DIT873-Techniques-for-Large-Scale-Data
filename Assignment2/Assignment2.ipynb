{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **DAT346/DIT873 Techniques for Large-Scale Data** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Assignment 2**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: group 23** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$$\\qquad$     **Julia Szulc, 961029-7245, juliasz@student.chalmers.se** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$$\\qquad$     **Ahmed Groshar, 961222-1912, gusgroah@student.gu.se** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Speedup measurements for accuracy 0.00001:\n",
    "<img src=\"files/imgs/speedup_mc_1e-5.png\">\n",
    "The spike of a super-linear speedup could be caused by a possible caching effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "This section describes a higher pseudo-code view of the k-means implementation we try to parallelize:\n",
    "\n",
    "    def nearestCentroid(datum, centroids):\n",
    "        # function computing distance from data point to centroids\n",
    "\n",
    "    def evaluate_results(clustering_results, k):\n",
    "        # combine new centroid values for each centroid\n",
    "\n",
    "\n",
    "    def workerTask(worker_data):\n",
    "\n",
    "        # compute nearest centroid to each chunk dataset\n",
    "        # compute partial new centriod \n",
    "        for i in range(N_data_points_chunk):\n",
    "                cluster, dist = nearestCentroid(data[i],centroids)\n",
    "                ...\n",
    "                new_centroids[cluster] += data_chunk[i]\n",
    "\n",
    "\n",
    "    def kmeans(k, data, nr_iter = 100):\n",
    "\n",
    "        # for a given number of iterations\n",
    "        for j in range(nr_iter):\n",
    "\n",
    "            # Give each worker a chunk dataset and centroids\n",
    "            processes = mp.Pool(workers)\n",
    "            clustering_results = processes.map(workerTask, workers_data)\n",
    "\n",
    "            # combine centroid values from workers\n",
    "            evaluate_results(clustering_results, k)\n",
    "\n",
    "\n",
    "\n",
    "The first two function nearestCentroid() and _evaluateresults() are helper functions that are called to compute the nearest centroid for a point and the second function to combine the results for the reassignment of new centroids. The first two function _nearestCentroid()_ and _evaluate_results()_ are helper functions that are called to compute the nearest centroid for a point and the second function to combine the results for the reassignment of new centroids. There are two main computations that are done in the k-means algorithm:\n",
    "1. compute the closest centriod for each point in the dataset\n",
    "2. recompute the next new centroids coordinates\n",
    "Then these two computations are repeated for _n_iter_ times.\n",
    "\n",
    "We succeeded to parallelize the 1 and 2 computation. When we have to repeat this these two computations for n_iter times any of the values that are obtained at each iteration depends on values of the iterations before, making it not possible to parallelize.\n",
    "\n",
    "The data communicated from the main thread to the worker threads is _data_chunk, centroids_ and from each worker thread back is _cluster_sizes, variation, new_centroids, assignments_chunk_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "The program was run with 50,000 samples. The measurements were made for a clustering part - assigning the classes to all the datapoints (*assigning part*) and for a centroids update (*update part*).\n",
    "\n",
    "| Part   |      Time [ms]      |\n",
    "|----------|:-------------:|\n",
    "| assigning |  693.5|\n",
    "| update |    64.6   |\n",
    "|||\n",
    "|**Total run time [ms]:**|75,830|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "**Ahmdal's Law:**\n",
    "$$ S_.total(f, S) = = \\frac{1}{(1 - f)+\\frac{f}{S}}$$\n",
    "\n",
    "__f__ is proportion of running time which can be accelerated by a factor of S\n",
    "\n",
    "__S total__ is the speedup of total run\n",
    "\n",
    "\n",
    "$$ S_.total(f, S) = = \\frac{1}{(1 - f)+\\frac{f}{S}}$$\n",
    "\n",
    "Using that, theoretical values of speedup for assigning and update parts were calculated for 4 and 8 cores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "Speedup measurements for 100,000 samples and 4 classes:\n",
    "<img src=\"files/imgs/speedup_kmeans_100k.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
